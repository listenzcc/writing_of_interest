\input{settings-all-in-one}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}{Proposition}[section]

\title{Normal Distribution}
\author{listenzcc}

\begin{document}

\maketitle

\abstract
The hand book of Normal distribution.
How it is formulated, what it can predict and why we use it.

\tableofcontents

\section{Normal Distribution in Classic View}

\subsection{Binominal Distribution}
Perform experiment for $n$ times, we assume the trials are independent and follow the same distribution.
The output of the experiment is noted as $1$ or $0$, with no vague.
The probability of observing the $1$ output is noted as $p$.
Then, we have
\begin{equation}
    \label{Equation: Binominal distribution}
    P(n, m) = (n, m) \cdot p^{m} \cdot (1-p)^{n-m}
\end{equation}
where $m$ refers the fact that we observe $1$ output for $m$ times.

It is easy to see that the $P(n, m)$ produces a distribution since
\begin{equation*}
    \sum_{i=0}^{n} P(n, i) = 1, i \in \mathcal{N}
\end{equation*}

Use the computation of \textbf{expectation and Variation} of the random variable, we have
\begin{align*}
    \mathcal{E} (m) & = n \cdot p             \\
    \mathcal{V} (m) & = n \cdot p \cdot (1-p)
\end{align*}

\subsection{Poisson Distribution}
The poisson distribution is the infinity binominal distribution (see \eqref{Equation: Binominal distribution}),
when $p$ is \textbf{small} and $n$ is \textbf{large}.
It is defined as
\begin{equation}
    \label{Equation: Poisson distribution}
    P(k) = \frac{\lambda^k}{k!} \cdot e^{-\lambda}, \lambda=np
\end{equation}

\begin{proof}
    Use the equation of $\lambda = np$, we can rewrite \eqref{Equation: Binominal distribution} as
    \begin{equation*}
        P(n, k) = (n, k) (\frac{\lambda}{n})^k (1-\frac{\lambda}{n})^{n-k}
    \end{equation*}

    Since $p$ is small and $n$ is large, we have $k$ is relatively small compared to $n$.
    As a result, in infinity case,
    \begin{align*}
        \lim_{n \rightarrow \infty} \frac{(n, k)}{n^k}          & = \frac{1}{k!}   \\
        \lim_{n \rightarrow \infty} (1-\frac{\lambda}{n})^{n-k} & = e ^ {-\lambda}
    \end{align*}

    Hence proved.
\end{proof}

In practice, we require $\lambda < 1$ to produce a valid approximation.

To compute the expectation and variation of the poisson distribution, we use the taylor series of Exp function
\begin{equation*}
    e^{\lambda t} = \sum_{k=0}^{\infty} \frac{\lambda^k}{k!}, t_0 = 0
\end{equation*}
it naturally guarantees the property of PDF that
\begin{equation*}
    \sum_{k=0}^{\infty} P(k) = 1
\end{equation*}

Use the definition of the expectation, we have
\begin{align*}
    \mathcal{E} (k) & = \sum_{k=1}^{\infty} \frac{\lambda^k}{(k-1)!} \cdot e^{-\lambda} \\
    \mathcal{E} (k) & = \lambda
\end{align*}

Use the definition of the variation, we have
\begin{align*}
    \mathcal{E} (k^2) & = \sum_{k=1}^{\infty} \frac{k \lambda^k}{(k-1)!} \cdot e^{-\lambda}         \\
    \mathcal{E} (k^2) & = \lambda + \sum_{k=2}^{\infty} \frac{\lambda^k}{(k-2)!} \cdot e^{-\lambda} \\
    \mathcal{E} (k^2) & = \lambda + \lambda^2
\end{align*}
where we used the idea of $k = 1 + (k-1)$.
Thus, the variation is
\begin{equation*}
    \mathcal{V} (k) = \lambda
\end{equation*}

\subsection{Normal Distribution}
When $n$ is large and $p$ is not so small, the poisson distribution fails on approximate the binominal distribution.
The Normal distribution is used as a more general replacement.

Basically, when $n$, $np$ and $nq$ are large, the binominal distribution is well approximated by the Normal distribution
\begin{equation*}
    p(x) = (n, x) p^x q^{n-x} \approx
    \frac{1}{\sqrt{2 \pi n p q}}
    e^{-(x-np)^2/2npq}
\end{equation*}
where $p+q=1$.
See the website \footnote{\url{http://scipp.ucsc.edu/~haber/ph116C/NormalApprox.pdf}} for detail.

And they are linked based on Sterling's formula,
\begin{equation}
    \label{Equation: Sterling's formula}
    n! = n^n e^{-n} \sqrt{2 \pi n} [1 + \mathcal{O}(1/n)]
\end{equation}
See the website \footnote{\url{https://www.researchgate.net/publication/237571154_A_Very_Short_Proof_of_Stirling's_Formula}} for detail.

Formally, the Normal distribution is expressed as
\begin{equation}
    \label{Equation: Pdf of Normal distribution}
    p(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2 \sigma^2}}
\end{equation}
where expectation $\mu = \lim_{n \rightarrow \infty} np$ and variation $\sigma^2 = \lim_{n \rightarrow \infty} npq$.

\subsubsection{The Pdf of Normal Distribution}
The Normal distribution is usually expressed as $p(x) \sim \mathcal{N}(\mu, \sigma^2)$.

\begin{proof}
    The Pdf of Normal distribution is a pdf.
    \begin{align*}
        \int_{-\infty}^{\infty} p(x) & = 1, p(x) > 0                   \\
        p(x)                         & \sim \mathcal{N}(\mu, \sigma^2)
    \end{align*}

    Firstly, the $p(x) > 0$ is obvious.
    Secondly, using the equation of $\Gamma(\frac{1}{2}) = \sqrt{\pi}$,
    the integral is $1$.

    Hence proved.
\end{proof}

The expectation and variance are
\begin{align*}
    \mathcal{E}(x) & = \mu      \\
    \mathcal{V}(x) & = \sigma^2
\end{align*}

\begin{proof}
    The expectation and variance of Normal distribution are $\mu$ and $\sigma^2$.

    Use the definition of expectation, and variable change with $y = x - \mu$, one gets
    \begin{equation*}
        \mathcal{E}(x) = \int_{-\infty}^{\infty} (y+\mu) p(y+\mu) dy
    \end{equation*}

    Since integrand function of $y \cdot p(y+\mu)$ is odd function,
    and use the PDF property of Normal distribution,
    one gets $\mathcal{E}(x) = \mu$.

    Use the value of $\Gamma(\frac{3}{2}) = \frac{\sqrt{\pi}}{2}$,
    and variable change with $y = \frac{x - \mu}{\sqrt{2 \sigma^2}}$,
    one gets
    \begin{equation*}
        \mathcal{V}(x) = \int_{-\infty}^{\infty}
        (x - \mu)^2 p(x) dx
        = \sigma^2
    \end{equation*}

    Hence proved.
\end{proof}

\section{Family Members}
There are several common distributions in the family of Normal distribution.

\subsection{Chi-squared Distribution}
If $Y_i \sim \mathcal{N}(0, 1)$, and $Y_i$s are independent with each other, then
\begin{equation}
    \chi^2 \equiv \sum_{i = 1}^{r} Y_i^2
\end{equation}
is distributed as Chi-squared \emph{$\chi^2$} distribution with $r$ degrees of freedom.
The symbolic notion is $p_r(x) \sim \chi^2(r)$.

The pdf of Chi-squared distribution is
\begin{equation}
    \label{Equation: Pdf of Chi-squared Distribution}
    p_r(x) = \frac{x^{r/2-1} e^{-x/2}}{\Gamma(r/2) 2^{r/2}}, 0 < x < \infty
\end{equation}

\subsubsection{Mean and Variance}

The mean and variance of the chi-squared distribution is
\begin{align*}
    Mean     & \triangleq E(x) = r             \\
    Variance & \triangleq E(x^2) - E^2(x) = 2r
\end{align*}

\subsubsection{The Pdf of Chi-squared distribution}
\begin{lemma}
    \label{lemma: Compute the pdf of Chi-squared distribution}
    To get the pdf of a Chi-squared distribution, we have to prove that
    \begin{equation*}
        p_{n}(x) \propto x^{n/2-1} \cdot e^{-x/2}
    \end{equation*}
    in which, $x = \sum_{i=1}^{n} y_i^2$ and $y_i \sim \mathcal{N}(0, 1)$.
    Each $y_i$ are independent.

\end{lemma}

\begin{proof}
    The joint probability of $\{y_1, y_2, \dots, y_n\}$ is
    \begin{equation*}
        p_{joint} = exp(\sum_{i=1}^{n}-y_i^2/2)
    \end{equation*}

    Thus, the cumulative sum of $p_n(x)$ can be computed using surface integral
    \begin{align*}
        P_n(r<\sqrt{x}) & \propto \int_{S} p_{joint} ds  \\
        P_n(r<\sqrt{x}) & \propto \int_{S} e^{-r^2/2} ds
    \end{align*}
    in which, $S$ refers the volume of a sphere with radius of $x$.

    Transfer the integral into sphere coordinates, we have
    \begin{equation*}
        P_n(r<\sqrt{x}) \propto \int_{r=0}^{\sqrt{x}} e^{-r^2/2} r^{(n-1)} dr
    \end{equation*}

    Derivate to $x$, we have
    \begin{align*}
        \frac{\partial}{\partial{x}} {P_n(r<\sqrt{x})} & \propto e^{-r^2/2} r^{(n-1)} x^{-1/2} \\
        \frac{\partial}{\partial{x}} {P_n(r<\sqrt{x})} & \propto x^{n/2-1} \cdot e^{-x/2}
    \end{align*}
    The first step is because of the Newton's integral rule, the second step is based on the replacement of $r = \sqrt{x}$.

    Hence proved.

\end{proof}

\begin{lemma}
    \label{lemma: The Pdf of Chi-squared Distribution is a Pdf}
    Next, we have to prove that the integral of $p_n(x)$ with $p_n(x) \sim \chi^2(n)$ is
    \begin{equation*}
        \int_0^\infty p_n(x) dx = \Gamma(n/2) \cdot 2^{r/2}
    \end{equation*}

\end{lemma}

\begin{proof}
    Use the definition of $\Gamma$ function
    \begin{equation*}
        \Gamma(n) = \int_0^\infty x^{n-1} e^{-x} dx
    \end{equation*}

    Use variable replacement of $z = 2x$, we have
    \begin{equation*}
        \Gamma(n) = 2^{-n} \int_0^\infty z^{n-1} e^{-z/2} dz
    \end{equation*}

    Then, use substitution of $n = n/2$, we have
    \begin{equation*}
        \Gamma(n/2) \cdot 2^{n/2} = \int_0^\infty z^{n/2-1} e^{-z/2} dz
    \end{equation*}

    Hence proved.

\end{proof}

\subsection{Student's T Distribution}

The student's t distribution describes a random variable $T$ of the form
\begin{equation}
    T = \frac{\bar{x} - m}{s / \sqrt{N}}
\end{equation}
where $\bar{x}$ is the sample mean value of all $N$ samples, $m$ is the population mean value and $s$ is the population standard deviation.

Or, in a more formal one
\begin{equation}
    T = \frac{X}{\sqrt{Y/r}}
\end{equation}
where $X \sim \mathcal{N}(0, 1)$ and $Y \sim \chi_r^2$.

The pdf of Student's t-distribution is
\begin{equation}
    \label{Equation: Pdf of Student's T Distribution}
    t_r(x) = \frac{\Gamma(\frac{r+1}{2})}{\Gamma(\frac{r}{2}) \sqrt{r\pi}} (1+\frac{x^2}{r})^{-\frac{r+1}{2}}, -\infty < x < \infty
\end{equation}

\subsubsection{Mean and Variance}

The mean and variance of the Student's t-distribution is
\begin{align*}
    Mean     & \triangleq E(x) = 0                        \\
    Variance & \triangleq E(x^2) - E^2(x) = \frac{r}{r-2}
\end{align*}

\subsubsection{Relationship with Normal Distribution}

It is easy to see that $\lim_{r \to \infty} t_r(x) \sim \mathcal{N}(0, 1)$.
It demonstrates that when $r$ is large enough, the Student's t-distribution is equalize to Normal Distribution.
It should to be noted that the calculation of $\frac{\Gamma(\frac{r+1}{2})}{\Gamma{\frac{r}{2}} \sqrt{r \pi}}$ is somehow difficult,
however, and fortunately, the value is constant with $x$ when $r \rightarrow \infty$.
Since the other factor can be formulated as the form of $e^\frac{x^2}{2}$,
the constant can be calculated using the property of Normal distribution.
Thus, the equation is also an useful approximation to the constant.

\subsubsection{The pdf of Student's T Distribution}

Here, we provide a simple computation of the pdf of the Student's t-distribution.
\begin{equation*}
    T=\frac{X}{\sqrt{Y/r}}
\end{equation*}
in which $X \sim \mathcal{N}(0, 1)$ and $Y \sim \chi^2(r)$, and they are independent.
Thus, we have
\begin{align*}
    p(x) & \propto e^{-x^2/2}               \\
    p(y) & \propto y^{r/2-1} \cdot e^{-y/2}
\end{align*}

The random variable $t$ follows the equation $t=\frac{x}{\sqrt{y/r}}$.

\begin{lemma} \label{lemma: Compute the Pdf of Student's T Distribution}
    Since then we want to prove that
    \begin{equation}
        p(t) \propto (1+\frac{t^2}{r})^{-\frac{r+1}{2}}
    \end{equation}

\end{lemma}

\begin{proof}
    The joint probability of $p(x, y)$ matches
    \begin{equation*}
        p(x, y) \propto e^{-x^2/2} \cdot y^{r/2-1} \cdot e^{-y/2}
    \end{equation*}

    And the divergence of $p(x, y)$ is $p(x, y) dx dy$.
    We can use the variable replacement of
    \begin{align*}
        y             & = \frac{x^2}{t^2} \cdot r \\
        \frac{dy}{dt} & \propto \frac{x^2}{t^3}
    \end{align*}

    Thus we have the joint probability of $p(x, t)$ matches
    \begin{equation*}
        p(x, t) \propto e^{-x^2/2} \cdot (\frac{x^2}{t^2})^{r/2-1} \cdot e^{-\frac{x^2}{2t^2}r} \cdot \frac{x^2}{t^3}
    \end{equation*}

    The probability of $p(t)$ can be expressed as
    \begin{equation*}
        p(t) \propto \int_{x} p(x, t) dx
    \end{equation*}

    Analysis the expression, we have
    \begin{align*}
        p(t) & \propto t^{-r-1} \int_{x} x^{r} \cdot e^{-\frac{1}{2}(1+\frac{r}{t^2})x^2} dx           \\
        p(t) & \propto t^{-r-1} \cdot (1+\frac{r}{t^2})^\frac{-r-1}{2} \int_{z} z^{r} \cdot e^{z^2} dz \\
        p(t) & \propto (t^2 + r) ^ {-\frac{r+1}{2}}                                                    \\
        p(t) & \propto (1+\frac{t^2}{r})^{-\frac{r+1}{2}}
    \end{align*}

    The process uses the integral of $\Gamma$ function is constant, and $r$ is constant.
\end{proof}


After that, combining with the following, we should finally have the pdf function.

\begin{lemma}
    \label{lemma: Pdf of Student's T Distribution is a Pdf}
    The values of $t_r(x)$ is positive and the integral is $1$.
    \begin{equation*}
        \int_{-\infty}^{\infty} t_r(x) \,\mathrm{d}x = 1
    \end{equation*}

\end{lemma}

\begin{proof}
    Consider the variable part of Student's t-distribution
    \begin{equation*}
        f(x) = (1+\frac{x^2}{r})^{-\frac{r+1}{2}}, -\infty < x < \infty
    \end{equation*}

    use a replacement as following
    \begin{equation*}
        x^2 = \frac{y}{1-y}
    \end{equation*}
    it is easy to see that $\lim_{y \to 0} x = 0$ and $\lim_{y \to 1} x = \infty$.
    Additionally, the $x^2$ is even function.
    Thus we can write the integral of $f(x)$
    \begin{equation*}
        \int_{-\infty}^{\infty} f(x) \,\mathrm{d}x =
        2 \sqrt{r} \int_{0}^{1} (\frac{1}{1-y})^{-\frac{r+1}{2}} \,\mathrm{d} (\frac{y}{1-y})^\frac{1}{2}
    \end{equation*}
    it is not hard to find out that the integral may end up with
    \begin{equation*}
        \sqrt{r} \int_{0}^{1} (1-y)^{\frac{r}{2}-1} y^{\frac{1}{2}-1} \,\mathrm{d}y =
        \sqrt{r} B(\frac{r}{2}, \frac{1}{2})
    \end{equation*}

    Finally the Normalization factor has to be
    \begin{equation*}
        \frac{\Gamma(\frac{r+1}{2})}{\sqrt{r} \Gamma(\frac{r}{2}) \Gamma(\frac{1}{2})}
    \end{equation*}
    which makes the integral of $t_r(x)$ is $1$.

\end{proof}


\section{Examples}

One important usage of normal distribution is to valuate the output of the experiment.
Since a large number of observations can be fitted into a normal distribution.

In an experiment, the results can be variance on different experiment trails.
In the standard analysis pipeline,
the \emph{random variables} which can be fitted into the normal distribution,
are formulated based on the obtained outputs.

The statistical analysis is commonly used to obtain the underlying ground truth value of the outputs.
Particularly, we are interested in the \emph{expectation} and \emph{variance} of the random variables.
The expectation is used to estimate the ideal value, often refers noise-free situation, of one trial.
The variance is used to estimate the reasonable range of the output of one trial.
Additionally, the distribution of the selected random variable is also used to \emph{valuate} or \emph{compare} the output of the experiment.

\subsection{Paired and Un-paired T-test}

The t-test method is the useful method to compare the effects of \emph{two} factors.
A common situation is to compare the difference of two methods doing the same job.
We assume the outputs can be evaluated as the random variable fitting in \emph{student's t distribution}.
Then the sum of the variables is also fitting in \emph{student's t distribution},
but the only question is the parameters being left as unknown.

To perform valid comparison of the variables of two methods,
the \emph{generation} of the random variables and the \emph{estimation} of the parameters should be investigated.
Back to the question of binary comparison.
If for each experiment trial, we can parallel apply the two methods to it, the \emph{paired t-test} method will be used.
If we can only apply the method on a trial once, the \emph{un-paired t-test} method will be used.

\subsubsection{Paired T-test}

\subsubsection{Un-paired T-test}

\subsection{Required Sample Numbers for Solid Output}

\subsection{Analysis of Variance}

\end{document}